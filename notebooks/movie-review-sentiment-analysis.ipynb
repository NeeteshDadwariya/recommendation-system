{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from nltk import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['text', 'sentiment'])\n",
    "\n",
    "for id in movie_reviews.fileids():\n",
    "  text = ' '.join(movie_reviews.words(id))\n",
    "  sentiment = 1 if movie_reviews.categories(id) == 'pos' else 0\n",
    "  data = data.append(pd.DataFrame({'text': text,'sentiment': sentiment}, index=[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "# remove punctuation from data\n",
    "clean = [re.sub(r'[^\\w\\s]','',i).lower() for i in data]\n",
    "\n",
    "tokens = [word_tokenize(x) for x in data['text']]\n",
    "filtered_tokens = []\n",
    "\n",
    "# tokens that are not stopwords collected here\n",
    "for i in tokens:\n",
    "  filtered_tokens.append([])\n",
    "  for j in i:\n",
    "    if j in en_stopwords:\n",
    "      continue\n",
    "    else: filtered_tokens[-1].append(j)\n",
    "\n",
    "# initialize Lancaster Stemmer\n",
    "LS = LancasterStemmer()\n",
    "lemmatized = []\n",
    "for l in filtered_tokens: lemmatized.append([LS.stem(w) for w in l])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Data analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Data analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "flat_list = [i for sublist in filtered_tokens for i in sublist]\n",
    "\n",
    "# Count how many times each word appears\n",
    "count = Counter(flat_list).items()\n",
    "sorted_count = sorted(count, key = itemgetter(1))\n",
    "sorted_count.reverse()\n",
    "\n",
    "# Select 5000 most frequent words\n",
    "top5000 = [i[0] for i in sorted_count[:5000]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Extracting sentiments for the words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_matrix = []\n",
    "\n",
    "for i in lemmatized: word_matrix.append([1 if j in i else 0 for j in top5000])\n",
    "features = pd.DataFrame(word_matrix, columns = top5000, index = pd.DataFrame(filtered_tokens))\n",
    "features['sentiment'] = data['sentiment'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pos_reviews = data[data['sentiment'] == 1]\n",
    "neg_reviews = data[data['sentiment'] == 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Traing the model for the sentiments using Naive Bayers classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = train.columns[:-1]\n",
    "\n",
    "gnb = MultinomialNB()\n",
    "gnb.fit(train[cols], train['sentiment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Calculate accuracy and performance for reviews sentiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-system-dev",
   "language": "python",
   "name": "recommendation-system-dev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}